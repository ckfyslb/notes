# ML知识点

## 数据理解

## 数据清洗



## 超参数搜索方法

通过训练算法手动检查随机超参数集，并选择符合我们目标的最佳参数集。

### 1 网格搜索

类似于手动调优，为网格中指定的所有给定超参数值的每个排列构建模型，评估并选择最佳模型。

```python
from sklearn.model_selection import GridSearchCV

knn = KNeighborsClassifier()
grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }
              
grid = GridSearchCV(knn,grid_param,cv = 5)
grid.fit(X_train,y_train)

#best parameter combination
grid.best_params_

#Score achieved with best parameter combination
grid.best_score_

#all combinations of hyperparameters
grid.cv_results_['params']

#average scores of cross-validation
grid.cv_results_['mean_test_score']
```

**缺点**：

由于它尝试了超参数的每一个组合，并根据交叉验证得分选择了最佳组合，这使得GridsearchCV非常慢。

### 2 随机搜索

使用随机搜索代替网格搜索的动机是，在许多情况下，所有的超参数可能不是同等重要的。随机搜索从超参数空间中随机选择参数组合，参数由n_iter给定的固定迭代次数的情况下选择。实验证明，随机搜索的结果优于网格搜索。

```python
from sklearn.model_selection import RandomizedSearchCV

knn = KNeighborsClassifier()

grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }

rand_ser = RandomizedSearchCV(knn,grid_param,n_iter=10)
rand_ser.fit(X_train,y_train)

#best parameter combination
rand_ser.best_params_

#score achieved with best parameter combination
rand_ser.best_score_

#all combinations of hyperparameters
rand_ser.cv_results_['params']

#average scores of cross-validation
rand_ser.cv_results_['mean_test_score']
```

**缺点**：

随机搜索的问题是它不能保证给出最好的参数组合。

### 3 贝叶斯搜索

贝叶斯优化属于一类优化算法，称为基于序列模型的优化(SMBO)算法。这些算法使用先前对损失**f**的观察结果，以确定下一个(最优)点来抽样**f**。该算法大致可以概括如下。

1. 使用先前评估的点X1*:n*，计算损失f的后验期望。
2. 在新的点X的抽样损失f，从而最大化f的期望的某些方法。该方法指定f域的哪些区域最适于抽样。

重复这些步骤，直到满足某些收敛准则。

需要安装 scikit- optimization
pip install scikit-optimize

```python
from skopt import BayesSearchCV

import warnings
warnings.filterwarnings("ignore")

# parameter ranges are specified by one of below
from skopt.space import Real, Categorical, Integer

knn = KNeighborsClassifier()
#defining hyper-parameter grid
grid_param = { 'n_neighbors' : list(range(2,11)) , 
              'algorithm' : ['auto','ball_tree','kd_tree','brute'] }

#initializing Bayesian Search
Bayes = BayesSearchCV(knn , grid_param , n_iter=30 , random_state=14)
Bayes.fit(X_train,y_train)

#best parameter combination
Bayes.best_params_

#score achieved with best parameter combination
Bayes.best_score_

#all combinations of hyperparameters
Bayes.cv_results_['params']

#average scores of cross-validation
Bayes.cv_results_['mean_test_score']
```

另一个实现贝叶斯搜索的类似库是bayesian-optimization
pip install bayesian-optimization

**缺点**：

要在2维或3维的搜索空间中得到一个好的代理曲面需要十几个样本，增加搜索空间的维数需要更多的样本。

### 总结

在确定参数的最佳组合的保证和计算时间之间总是存在权衡。如果超参数空间(超参数个数)非常大，则使用随机搜索找到超参数的潜在组合，然后在该局部使用网格搜索(超参数的潜在组合)选择最优特征。

## 特征重要性分析方法

[Python中进行特征重要性分析的9个常用方法 (qq.com)](https://mp.weixin.qq.com/s/rERf19MMLzruHeW7O_G9gA)

特征重要性分析用于了解每个特征(变量或输入)对于做出预测的有用性或价值。目标是确定对模型输出影响最大的最重要的特征，它是机器学习中经常使用的一种方法。

如果有一个包含数十个甚至数百个特征的数据集，每个特征都可能对你的机器学习模型的性能有所贡献。但是并不是所有的特征都是一样的。有些可能是冗余的或不相关的，这会增加建模的复杂性并可能导致过拟合。

特征重要性分析可以识别并关注最具信息量的特征，从而带来以下几个优势:

- 改进的模型性能
- 减少过度拟合
- 更快的训练和推理
- 增强的可解释性

**1、排列重要性 PermutationImportance**

该方法会随机排列每个特征的值，然后监控模型性能下降的程度。如果获得了更大的下降意味着特征更重要

**2、内置特征重要性(coef_或feature_importances_)**

一些模型，如线性回归和随机森林，可以直接输出特征重要性分数。这些显示了每个特征对最终预测的贡献。

**3、Leave-one-out**

迭代地每次删除一个特征并评估准确性。

**4、相关性分析**

计算各特征与目标变量之间的相关性。相关性越高的特征越重要。

**5、递归特征消除 Recursive Feature Elimination**

递归地删除特征并查看它如何影响模型性能。删除时会导致更大下降的特征更重要。

**6、XGBoost特性重要性**

计算一个特性用于跨所有树拆分数据的次数。更多的分裂意味着更重要。

**7、主成分分析 PCA**

对特征进行主成分分析，并查看每个主成分的解释方差比。在前几个组件上具有较高负载的特性更为重要。

**8、方差分析 ANOVA**

使用f_classif()获得每个特征的方差分析f值。f值越高，表明特征与目标的相关性越强。

**9、卡方检验**

使用chi2()获得每个特征的卡方统计信息。得分越高的特征越有可能独立于目标。

### 为什么不同的方法会检测到不同的特征?

不同的特征重要性方法有时可以识别出不同的特征是最重要的，这是因为：

**1、他们用不同的方式衡量重要性:**

有的使用不同特特征进行预测，监控精度下降

像XGBOOST或者回国模型使用内置重要性来进行特征的重要性排列

而PCA着眼于方差解释

**2、不同模型有不同模型的方法：**

线性模型倾向于线性关系、树模型倾向于接近根的特征

**4、交互作用:**

有的方法可以获取特征之间的相互左右，而有一些则不行，这就会导致结果的差异

**5、不稳定:**

使用不同的数据子集，重要性值可能在同一方法的不同运行中有所不同，这是因为数据差异决定的

**6、Hyperparameters:**

通过调整超参数，如PCA组件或树深度，也会影响结果

所以不同的假设、偏差、数据处理和方法的可变性意味着它们并不总是在最重要的特征上保持一致。

### 选择特征重要性分析方法的一些最佳实践

- 尝试多种方法以获得更健壮的视图
- 聚合结果的集成方法
- 更多地关注相对顺序，而不是绝对值
- 差异并不一定意味着有问题，检查差异的原因会对数据和模型有更深入的了解



## 正则化

==正则化通过在模型的损失函数中引入额外的惩罚项，来对模型的参数进行约束，从而降低模型的复杂度，防止模型出现过拟合。==

### 正则化的作用

**防止过拟合**：正则化通过对模型的复杂度进行限制，防止模型在训练数据上过度拟合。过拟合指的是模型在训练数据上表现良好，但在未见过的数据上表现较差的情况，这可能是因为模型学习到了训练数据中的噪声或者细节，而无法泛化到新数据上。正则化有助于使模型更加简单，从而提高其在未见过的数据上的泛化能力。

**提高模型的泛化能力**：正则化约束了模型的复杂度，使其更容易泛化到未见过的数据上。通过控制模型的参数大小或数量，正则化可以使模型更加稳定，减少对训练数据的过度依赖，从而提高模型的泛化能力。

**减少模型的复杂度**：正则化技术通过对模型的参数进行惩罚，促使模型更趋向于简单的解。例如，L1 和 L2 正则化会约束模型的权重，使其趋向于稀疏或较小的值，从而减少模型的复杂度。

**控制模型的学习速度**：正则化技术可以对模型的学习速度进行调节，防止模型在训练过程中权重变化过大，从而导致优化过程不稳定。这有助于加速模型的收敛，并提高模型在训练数据上的表现。

**提高模型的鲁棒性**：正则化有助于使模型更加鲁棒，即对输入数据的微小变化不敏感。通过降低模型的复杂度，正则化可以减少模型对训练数据中噪声的敏感度，从而提高模型的鲁棒性。

### 常见的正则化方法

**L1 正则化**：也称为 Lasso 正则化，它通过在模型的损失函数中增加权重的 L1 范数（**权重向量的绝对值之和**）来实现正则化。**L1 正则化倾向于产生稀疏权重矩阵，即将一些权重推向零，从而实现特征选择的效果**。

![image-20240618133340704](C:\Users\CZY\AppData\Roaming\Typora\typora-user-images\image-20240618133340704.png)

**L2 正则化**：也称为 Ridge 正则化，它通过在模型的损失函数中增加权重的 L2 范数（**权重向量的平方和**）来实现正则化。**L2 正则化会使权重值变得较小，但不会直接导致权重稀疏，因此不具有特征选择的作用，但可以有效地控制模型的复杂度**。

![image-20240618133350864](C:\Users\CZY\AppData\Roaming\Typora\typora-user-images\image-20240618133350864.png)

**Elastic Net 正则化（弹性网）**：Elastic Net 是 L1 和 L2 正则化的组合，它在损失函数中同时使用 L1 和 L2 范数，可以综合两者的优点。

**Dropout**：Dropout 是一种特殊的正则化技术，通过在训练过程中随机地丢弃（将其权重置为零）网络中的部分神经元，以及它们的连接，来减少神经网络的复杂度。这样可以防止神经元之间的共适应性，从而减少过拟合。

**早停（Early Stopping）**：早停是一种简单而有效的正则化方法，它在训练过程中监视模型在验证集上的性能，一旦验证集上的性能开始下降，就停止训练。这样可以避免模型在训练集上过拟合。

**数据增强（Data Augmentation）**：数据增强是通过对训练数据进行变换来增加数据的多样性，从而减少过拟合的风险。例如，在图像分类任务中可以进行随机裁剪、旋转、翻转等操作来增加训练数据的数量和多样性。

**批量归一化（Batch Normalization）**：批量归一化是一种通过对每个批次的输入进行归一化来加速训练并减少过拟合的技术。它可以使得每一层的输入分布稳定，从而更容易优化模型。

**权重衰减（Weight Decay）**：权重衰减是一种通过在损失函数中增加权重的平方和或绝对值之和来实现正则化的技术。它等价于对权重参数进行 L2 正则化。

**LP范数**

**L1范数**

**L2范数**

**L1范数和L2范数的区别**

**Dropout**

**Batch Normalization**

**归一化、标准化 & 正则化**



## 模型融合





# ML特征工程和优化方法

## 1. 特征工程有哪些？

特征工程，顾名思义，是对原始数据进行一系列工程处理，将其提炼为特征，作为输入供算法和模型使用。从本质上来讲，特征工程是一个表示和展现数 据的过程。在实际工作中，**特征工程旨在去除原始数据中的杂质和冗余**，设计更高效的特征以刻画求解的问题与预测模型之间的关系。

主要讨论以下两种常用的数据类型。

1. 结构化数据。结构化数据类型可以看作关系型数据库的一张表，每列都 有清晰的定义，包含了数值型、类别型两种基本类型；每一行数据表示一个样本 的信息。
2. 非结构化数据。非结构化数据主要包括文本、图像、音频、视频数据， 其包含的信息无法用一个简单的数值表示，也没有清晰的类别定义，并且每条数 据的大小各不相同。

### 1.1 特征归一化

为了消除数据特征之间的量纲影响，我们需要对特征进行归一化处理，使得 不同指标之间具有可比性。例如，分析一个人的身高和体重对健康的影响，如果 使用米（m）和千克（kg）作为单位，那么身高特征会在1.6～1.8m的数值范围 内，体重特征会在50～100kg的范围内，分析出来的结果显然会倾向于数值差别比 较大的体重特征。想要得到更为准确的结果，就需要进行特征归一化 （Normalization）处理，使各指标处于同一数值量级，以便进行分析。

对数值类型的特征做归一化可以将所有的特征都统一到一个大致相同的数值 区间内。最常用的方法主要有以下两种。

1. **线性函数归一化**（Min-Max Scaling）。它对原始数据进行线性变换，使 结果映射到[0, 1]的范围，实现对原始数据的等比缩放。归一化公式如下，其中*X*为原始数据， ![](https://latex.codecogs.com/gif.latex?X_{max}、X_{min})分别为数据最大值和最小值。 

   ![](https://latex.codecogs.com/gif.latex?X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}})

2. **零均值归一化**（Z-Score Normalization）。它会将原始数据映射到均值为 0、标准差为1的分布上。具体来说，假设原始特征的均值为μ、标准差为σ，那么 归一化公式定义为

   ![](https://latex.codecogs.com/gif.latex?z=\frac{x-u}{\sigma})

优点：**训练数据归一化后，容易更快地通过梯度下降找 到最优解。**

![](http://wx4.sinaimg.cn/mw690/00630Defly1g5cdl44ubjj30gz08i40j.jpg)

当然，数据归一化并不是万能的。在实际应用中，通过梯度下降法求解的模 型通常是需要归一化的，包括线性回归、逻辑回归、支持向量机、神经网络等模 型。但对于决策树模型则并不适用。

### 1.2 类别型特征

类别型特征（Categorical Feature）主要是指性别（男、女）、血型（A、B、 AB、O）等只在有限选项内取值的特征。类别型特征原始输入通常是字符串形 式，除了决策树等少数模型能直接处理字符串形式的输入，对于逻辑回归、支持 向量机等模型来说，类别型特征必须经过处理转换成数值型特征才能正确工作。

1. **序号编码**

   序号编码通常用于处理类别间具有大小关系的数据。例如成绩，可以分为 低、中、高三档，并且存在“高>中>低”的排序关系。序号编码会按照大小关系对 类别型特征赋予一个数值ID，例如高表示为3、中表示为2、低表示为1，转换后依 然保留了大小关系。

2. **独热编码(one-hot)**

   独热编码通常用于处理类别间不具有大小关系的特征。例如血型，一共有4个 取值（A型血、B型血、AB型血、O型血），独热编码会把血型变成一个4维稀疏 向量，A型血表示为（1, 0, 0, 0），B型血表示为（0, 1, 0, 0），AB型表示为（0, 0, 1, 0），O型血表示为（0, 0, 0, 1）。对于类别取值较多的情况下使用独热编码。

3. **二进制编码 **

   二进制编码主要分为两步，先用序号编码给每个类别赋予一个类别ID，然后 将类别ID对应的二进制编码作为结果。以A、B、AB、O血型为例，下图是二进制编码的过程。A型血的ID为1，二进制表示为001；B型血的ID为2，二进制表示为 010；以此类推可以得到AB型血和O型血的二进制表示。

   ![](http://wx1.sinaimg.cn/mw690/00630Defly1g5cdqz4zruj30lf07d74g.jpg)

### 1.3 高维组合特征的处理 

为了提高复杂关系的拟合能力，在特征工程中经常会把一阶离散特征两两组 合，构成高阶组合特征。以广告点击预估问题为例，原始数据有语言和类型两种 离散特征，第一张图是语言和类型对点击的影响。为了提高拟合能力，语言和类型可 以组成二阶特征，第二张图是语言和类型的组合特征对点击的影响。

![](http://wx3.sinaimg.cn/mw690/00630Defly1g5cdvbua1aj30n30kf752.jpg)

### 1.4 文本表示模型 

文本是一类非常重要的非结构化数据，如何表示文本数据一直是机器学习领 域的一个重要研究方向。

1. **词袋模型和N-gram模型**

   最基础的文本表示模型是词袋模型。顾名思义，就是将每篇文章看成一袋子 词，并忽略每个词出现的顺序。具体地说，就是将整段文本以词为单位切分开， 然后每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对 应的权重则反映了这个词在原文章中的重要程度。常用TF-IDF来计算权重。

2. **主题模型**

   主题模型用于从文本库中发现有代表性的主题（得到每个主题上面词的分布 特性），并且能够计算出每篇文章的主题分布。

3. **词嵌入与深度学习模型**

   词嵌入是一类将词向量化的模型的统称，核心思想是将每个词都映射成低维 空间（通常K=50～300维）上的一个稠密向量（Dense Vector）。K维空间的每一 维也可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

### 1.5 其它特征工程

1. 如果某个特征当中有**缺失值**，缺失比较少的话，可以使用该特征的平均值或者其它比较靠谱的数据进行填充；缺失比较多的话可以考虑删除该特征。
2. 可以分析特征与结果的相关性，把相关性小的特征去掉。

### 1.6 特征工程脑图

![](https://julyedu-img-public.oss-cn-beijing.aliyuncs.com/Public/Image/Question/1512980743_407.png)

## 2. 机器学习优化方法

优化是应用数学的一个分支，也是机器学习的核心组成部分。实际上，机器 学习算法 = 模型表征 + 模型评估 + 优化算法。其中，优化算法所做的事情就是在 模型表征空间中找到模型评估指标最好的模型。不同的优化算法对应的模型表征 和评估指标不尽相同。

### 2.1 机器学习常用损失函数

损失函数（loss function）是用来估量你模型的预测值f(x)与真实值Y的不一致程度，它是一个非负实值函数,通常使用L(Y, f(x))来表示，损失函数越小，模型的鲁棒性就越好。常见的损失函数如下：

1. **平方损失函数**

   ![](http://wx1.sinaimg.cn/mw690/00630Defly1g5e5tzcgisj30aa01hwed.jpg)

   Y-f(X)表示的是残差，整个式子表示的是残差的平方和，而我们的目的就是最小化这个目标函数值（注：该式子未加入正则项），也就是最小化残差的平方和。而在实际应用中，通常会使用均方差（MSE）作为一项衡量指标，公式如下：

   ![](https://latex.codecogs.com/gif.latex?MSE=\frac{1}{n}\sum_{i=1}^{n}(Y_i^{'}-Y_i)^2)

   该损失函数一般使用在线性回归当中。

2. **log损失函数**

   ![](https://wx1.sinaimg.cn/large/00630Defly1g4pvtz3tw9j30et04v0sw.jpg)

   公式中的 y=1 表示的是真实值为1时用第一个公式，真实 y=0 用第二个公式计算损失。为什么要加上log函数呢？可以试想一下，当真实样本为1是，但h=0概率，那么log0=∞，这就对模型最大的惩罚力度；当h=1时，那么log1=0，相当于没有惩罚，也就是没有损失，达到最优结果。所以数学家就想出了用log函数来表示损失函数。

   最后按照梯度下降法一样，求解极小值点，得到想要的模型效果。该损失函数一般使用在逻辑回归中。

   ![](http://wx2.sinaimg.cn/mw690/00630Defly1g5cf7z1k1rj30b40b4wej.jpg)

3. **Hinge损失函数**

   ![](https://latex.codecogs.com/gif.latex?L_i=\sum_{j\neq t_i}max(0,f(x_i,W)_j-(f(x_i,W)_{y_i}-\bigtriangleup)))

   SVM采用的就是Hinge Loss，用于“最大间隔(max-margin)”分类。

   ![](http://wx1.sinaimg.cn/mw690/00630Defly1g4w5ezjr64j30se03pmy6.jpg)

   详细见之前[SVM的文章1.2.3](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/4.%20SVM)

### 2.2 什么是凸优化

**凸函数**的严格定义为，函数L(·) 是凸函数当且仅当对定义域中的任意两点x，y和任意实数λ∈[0,1]总有：

![](http://wx4.sinaimg.cn/mw690/00630Defly1g5e5wisdtuj30d401ijra.jpg)

该不等式的一个直观解释是，凸函数曲面上任意两点连接而成的线段，其上的任 意一点都不会处于该函数曲面的下方，如下图所示所示。

![](http://wx4.sinaimg.cn/mw690/00630Defly1g5cfpms6woj30e1049wez.jpg)

凸优化问题的例子包括支持向量机、线性回归等 线性模型，非凸优化问题的例子包括低秩模型（如矩阵分解）、深度神经网络模型等。

### 2.3 正则化项

使用正则化项，也就是给loss function加上一个参数项，正则化项有**L1正则化、L2正则化、ElasticNet**。加入这个正则化项好处：

- 控制参数幅度，不让模型“无法无天”。
- 限制参数搜索空间
- 解决欠拟合与过拟合的问题。

详细请参考之前的文章：[线性回归--第5点](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/Liner%20Regression)

### 2.4 常见的几种最优化方法

1. **梯度下降法**

   梯度下降法是最早最简单，也是最为常用的最优化方法。梯度下降法实现简单，当目标函数是凸函数时，梯度下降法的解是全局解。一般情况下，其解不保证是全局最优解，梯度下降法的速度也未必是最快的。梯度下降法的优化思想是用当前位置负梯度方向作为搜索方向，因为该方向为当前位置的最快下降方向，所以也被称为是”最速下降法“。最速下降法越接近目标值，步长越小，前进越慢。梯度下降法的搜索迭代示意图如下图所示：

   ![](https://images2017.cnblogs.com/blog/1022856/201709/1022856-20170916201932735-243646199.png)

   缺点：靠近极小值时收敛速度减慢；直线搜索时可能会产生一些问题；可能会“之字形”地下降。

2. **牛顿法**

   牛顿法是一种在实数域和复数域上近似求解方程的方法。方法使用函数f (x)的泰勒级数的前面几项来寻找方程f (x) = 0的根。牛顿法最大的特点就在于它的收敛速度很快。具体步骤：

   - 首先，选择一个接近函数 f (x)零点的 x0，计算相应的 f (x0) 和切线斜率f  ' (x0)（这里f ' 表示函数 f  的导数）。

   - 然后我们计算穿过点(x0,  f  (x0)) 并且斜率为f '(x0)的直线和 x 轴的交点的x坐标，也就是求如下方程的解：

     ![](https://latex.codecogs.com/gif.latex?x*f^{'}(x_0)+f(x_0)-x_0*f^{'}(x_0)=0)

   - 我们将新求得的点的 x 坐标命名为x1，通常x1会比x0更接近方程f  (x) = 0的解。因此我们现在可以利用x1开始下一轮迭代。

   由于牛顿法是基于当前位置的切线来确定下一次的位置，所以牛顿法又被很形象地称为是"切线法"。牛顿法搜索动态示例图：

   ![](https://images2017.cnblogs.com/blog/1022856/201709/1022856-20170916202719078-1588446775.gif)

   从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。**缺点：**

   - 牛顿法是一种迭代算法，每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
   - 在高维情况下这个矩阵非常大，计算和存储都是问题。
   - 在小批量的情况下，牛顿法对于二阶导数的估计噪声太大。
   - 目标函数非凸的时候，牛顿法容易受到鞍点或者最大值点的吸引。

3. **拟牛顿法**

   拟牛顿法是求解非线性优化问题最有效的方法之一，**本质思想是改善牛顿法每次需要求解复杂的Hessian矩阵的逆矩阵的缺陷，它使用正定矩阵来近似Hessian矩阵的逆，从而简化了运算的复杂度。**拟牛顿法和梯度下降法一样只要求每一步迭代时知道目标函数的梯度。通过测量梯度的变化，构造一个目标函数的模型使之足以产生超线性收敛性。这类方法大大优于梯度下降法，尤其对于困难的问题。另外，因为拟牛顿法不需要二阶导数的信息，所以有时比牛顿法更为有效。如今，优化软件中包含了大量的拟牛顿算法用来解决无约束，约束，和大规模的优化问题。

4. **共轭梯度法**

   共轭梯度法是介于梯度下降法与牛顿法之间的一个方法，它仅需利用一阶导数信息，但克服了梯度下降法收敛慢的缺点，又避免了牛顿法需要存储和计算Hesse矩阵并求逆的缺点，共轭梯度法不仅是解决大型线性方程组最有用的方法之一，也是解大型非线性最优化最有效的算法之一。 在各种优化算法中，共轭梯度法是非常重要的一种。其优点是所需存储量小，具有步收敛性，稳定性高，而且不需要任何外来参数。

   具体的实现步骤请参加wiki百科[共轭梯度法](https://en.wikipedia.org/wiki/Conjugate_gradient_method#Example_code_in_MATLAB)。下图为共轭梯度法和梯度下降法搜索最优解的路径对比示意图：

   ![](http://wx2.sinaimg.cn/mw690/00630Defly1g5ch0r2p48j308z0almy4.jpg)

### 2.5 降维方法

#### 2.5.1 线性判别分析（LDA）

线性判别分析（Linear Discriminant Analysis，LDA）是一种经典的降维方法。和主成分分析PCA不考虑样本类别输出的无监督降维技术不同，LDA是一种监督学习的降维技术，数据集的每个样本有类别输出。  

LDA分类思想简单总结如下：  

1. 多维空间中，数据处理分类问题较为复杂，LDA算法将多维空间中的数据投影到一条直线上，将d维数据转化成1维数据进行处理。  
2. 对于训练数据，设法将多维数据投影到一条直线上，同类数据的投影点尽可能接近，异类数据点尽可能远离。  
3. 对数据进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定样本的类别。  

如果用一句话概括LDA思想，**即“投影后类内方差最小，类间方差最大”。**

假设有红、蓝两类数据，这些数据特征均为二维，如下图所示。我们的目标是将这些数据投影到一维，让每一类相近的数据的投影点尽可能接近，不同类别数据尽可能远，即图中红色和蓝色数据中心之间的距离尽可能大。

![](http://wx4.sinaimg.cn/mw690/00630Defgy1g5ma79ujplj30qr0ac3z8.jpg)

左图和右图是两种不同的投影方式。

​	左图思路：让不同类别的平均点距离最远的投影方式。

​	右图思路：让同类别的数据挨得最近的投影方式。

​	从上图直观看出，右图红色数据和蓝色数据在各自的区域来说相对集中，根据数据分布直方图也可看出，所以右图的投影效果好于左图，左图中间直方图部分有明显交集。

​	以上例子是基于数据是二维的，分类后的投影是一条直线。如果原始数据是多维的，则投影后的分类面是一低维的超平面。

**优缺点**

| 优缺点 | 简要说明                                                     |
| :----: | :----------------------------------------------------------- |
|  优点  | 1. 可以使用类别的先验知识；<br />2. 以标签、类别衡量差异性的有监督降维方式，相对于PCA的模糊性，其目的更明确，更能反映样本间的差异； |
|  缺点  | 1. LDA不适合对非高斯分布样本进行降维；<br />2. LDA降维最多降到分类数k-1维；<br />3. LDA在样本分类信息依赖方差而不是均值时，降维效果不好；<br />4. LDA可能过度拟合数据。 |

#### 2.5.2 主成分分析（PCA）

1. PCA就是将高维的数据通过线性变换投影到低维空间上去。
2. 投影思想：找出最能够代表原始数据的投影方法。被PCA降掉的那些维度只能是那些噪声或是冗余的数据。
3. 去冗余：去除可以被其他向量代表的线性相关向量，这部分信息量是多余的。
4. 去噪声，去除较小特征值对应的特征向量，特征值的大小反映了变换后在特征向量方向上变换的幅度，幅度越大，说明这个方向上的元素差异也越大，要保留。
5. 对角化矩阵，寻找极大线性无关组，保留较大的特征值，去除较小特征值，组成一个投影矩阵，对原始样本矩阵进行投影，得到降维后的新样本矩阵。
6. 完成PCA的关键是——协方差矩阵。协方差矩阵，能同时表现不同维度间的相关性以及各个维度上的方差。协方差矩阵度量的是维度与维度之间的关系，而非样本与样本之间。
7. 之所以对角化，因为对角化之后非对角上的元素都是0，达到去噪声的目的。对角化后的协方差矩阵，对角线上较小的新方差对应的就是那些该去掉的维度。所以我们只取那些含有较大能量(特征值)的维度，其余的就舍掉，即去冗余。

**图解PCA**

PCA可解决训练数据中存在数据特征过多或特征累赘的问题。核心思想是将m维特征映射到n维（n < m），这n维形成主元，是重构出来最能代表原始数据的正交特征。

​	假设数据集是m个n维，$(\boldsymbol x^{(1)}, \boldsymbol x^{(2)}, \cdots, \boldsymbol x^{(m)})$。如果$n=2$，需要降维到$n'=1$，现在想找到某一维度方向代表这两个维度的数据。下图有$u_1, u_2$两个向量方向，但是哪个向量才是我们所想要的，可以更好代表原始数据集的呢？

![](F:/jianguo_syc/GitHub/DeepLearning-500-questions-master/ch02_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80/img/ch2/2.34/1.png)

从图可看出，$u_1$比$u_2$好，为什么呢？有以下两个主要评价指标：

1. 样本点到这个直线的距离足够近。
2. 样本点在这个直线上的投影能尽可能的分开。

如果我们需要降维的目标维数是其他任意维，则：

1. 样本点到这个超平面的距离足够近。
2. 样本点在这个超平面上的投影能尽可能的分开。

**优缺点**

| 优缺点 | 简要说明                                                     |
| :----: | :----------------------------------------------------------- |
|  优点  | 1. 仅仅需要以方差衡量信息量，不受数据集以外的因素影响。　2.各主成分之间正交，可消除原始数据成分间的相互影响的因素。3. 计算方法简单，主要运算是特征值分解，易于实现。 |
|  缺点  | 1.主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。 |

#### 2.5.3 比较这两种方法

**降维的必要性**：

1. 多重共线性和预测变量之间相互关联。多重共线性会导致解空间的不稳定，从而可能导致结果的不连贯。
2. 高维空间本身具有稀疏性。一维正态分布有68%的值落于正负标准差之间，而在十维空间上只有2%。
3. 过多的变量，对查找规律造成冗余麻烦。
4. 仅在变量层面上分析可能会忽略变量之间的潜在联系。例如几个预测变量可能落入仅反映数据某一方面特征的一个组内。

**降维的目的**：

1. 减少预测变量的个数。
2. 确保这些变量是相互独立的。
3. 提供一个框架来解释结果。相关特征，特别是重要特征更能在数据中明确的显示出来；如果只有两维或者三维的话，更便于可视化展示。
4. 数据在低维下更容易处理、更容易使用。
5. 去除数据噪声。
6. 降低算法运算开销。

**LDA和PCA区别**

| 异同点 | LDA                                                          | PCA                                |
| :----: | :----------------------------------------------------------- | :--------------------------------- |
| 相同点 | 1. 两者均可以对数据进行降维；<br />2. 两者在降维时均使用了矩阵特征分解的思想；<br />3. 两者都假设数据符合高斯分布； |                                    |
| 不同点 | 有监督的降维方法；                                           | 无监督的降维方法；                 |
|        | 降维最多降到k-1维；                                          | 降维多少没有限制；                 |
|        | 可以用于降维，还可以用于分类；                               | 只用于降维；                       |
|        | 选择分类性能最好的投影方向；                                 | 选择样本点投影具有最大方差的方向； |
|        | 更明确，更能反映样本间差异；                                 | 目的较为模糊；                     |

## 3. 机器学习评估方法

混淆矩阵也称误差矩阵，是表示精度评价的一种标准格式，用n行n列的矩阵形式来表示。具体评价指标有总体精度、制图精度、用户精度等，这些精度指标从不同的侧面反映了图像分类的精度。下图为混淆矩阵

|          | 正类                | 负类                |
| -------- | ------------------- | ------------------- |
| 预测正确 | TP(True Positives)  | FP(False Positives) |
| 预测错误 | FN(False Negatives) | TN(True Negatives)  |

### 3.1 准确率(Accuracy)

**准确率（Accuracy）。**顾名思义，就是所有的预测正确（正类负类）的占总的比重。

![](https://latex.codecogs.com/gif.latex?Accuracy=\frac{TP+TN}{TP+TN+FP+FN})

准确率是分类问题中最简单也是最直观的评价指标，但存在明显的缺陷。比 如，当负样本占99%时，分类器把所有样本都预测为负样本也可以获得99%的准确 率。所以，当不同类别的样本比例非常不均衡时，占比大的类别往往成为影响准 确率的最主要因素。

### 3.2 精确率（Precision）

**精确率（Precision）**，查准率。即正确预测为正的占全部预测为正的比例。个人理解：真正正确的占所有预测为正的比例。

![](https://latex.codecogs.com/gif.latex?Precision=\frac{TP}{TP+FP})

### 3.3 召回率(Recall)

**召回率（Recall）**，查全率。即正确预测为正的占全部实际为正的比例。个人理解：真正正确的占所有实际为正的比例。

![](https://latex.codecogs.com/gif.latex?Recall=\frac{TP}{TP+FN})

为了综合评估一个排序模型的好坏，不仅要看模型在不同 Top N下的Precision@N和Recall@N，而且最好绘制出模型的P-R（Precision- Recall）曲线。这里简单介绍一下P-R曲线的绘制方法。

P-R曲线的横轴是召回率，纵轴是精确率。对于一个排序模型来说，其P-R曲 线上的一个点代表着，在某一阈值下，模型将大于该阈值的结果判定为正样本， 小于该阈值的结果判定为负样本，此时返回结果对应的召回率和精确率。整条P-R 曲线是通过将阈值从高到低移动而生成的。下图是P-R曲线样例图，其中实线代表 模型A的P-R曲线，虚线代表模型B的P-R曲线。原点附近代表当阈值最大时模型的 精确率和召回率。

![](http://wx2.sinaimg.cn/mw690/00630Defly1g5e4ocvl8aj30ep0d275m.jpg)

由图可见，当召回率接近于0时，模型A的精确率为0.9，模型B的精确率是1， 这说明模型B得分前几位的样本全部是真正的正样本，而模型A即使得分最高的几 个样本也存在预测错误的情况。并且，随着召回率的增加，精确率整体呈下降趋 势。但是，当召回率为1时，模型A的精确率反而超过了模型B。**这充分说明，只用某个点对应的精确率和召回率是不能全面地衡量模型的性能，只有通过P-R曲线的 整体表现，才能够对模型进行更为全面的评估。**

### 3.4 F1值(H-mean值)

F1值（H-mean值）。F1值为算数平均数除以几何平均数，且越大越好，将Precision和Recall的上述公式带入会发现，当F1值小时，True Positive相对增加，而false相对减少，即Precision和Recall都相对增加，即F1对Precision和Recall都进行了加权。

![](https://latex.codecogs.com/gif.latex?\frac{2}{F_1}=\frac{1}{Precision}+\frac{1}{Recall})

![](https://latex.codecogs.com/gif.latex?F_1=\frac{2PR}{P+R}=\frac{2TP}{2TP+FP+FN})

### 3.4 ROC曲线

ROC曲线。接收者操作特征曲线（receiver operating characteristic curve），是反映敏感性和特异性连续变量的综合指标，ROC曲线上每个点反映着对同一信号刺激的感受性。下图是ROC曲线例子。

![](http://wx2.sinaimg.cn/mw690/00630Defly1g5e4fnjx1dj308w08waby.jpg)

横坐标：1-Specificity，伪正类率(False positive rate，FPR，FPR=FP/(FP+TN))，预测为正但实际为负的样本占所有负例样本的比例；

纵坐标：Sensitivity，真正类率(True positive rate，TPR，TPR=TP/(TP+FN))，预测为正且实际为正的样本占所有正例样本的比例。

**真正的理想情况**，TPR应接近1，FPR接近0，即图中的（0,1）点。**ROC曲线越靠拢（0,1）点，越偏离45度对角线越好**。

**AUC值**

AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。

从AUC判断分类器（预测模型）优劣的标准：

- AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
- 0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
- AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
- AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。

**一句话来说，AUC值越大的分类器，正确率越高。**

### 3.5 余弦距离和欧式距离

**余弦距离：** ![](https://latex.codecogs.com/gif.latex?cos(A,B)=\frac{A*B}{||A||_2||B||_2})

**欧式距离：**在数学中，欧几里得距离或欧几里得度量是欧几里得空间中两点间“普通”（即直线）距离。

对于两个向量A和B，余弦距离关注的是向量之间的角度关系，并不关心它们的绝对大小，其取值 范围是[−1,1]。当一对文本相似度的长度差距很大、但内容相近时，如果使用词频 或词向量作为特征，它们在特征空间中的的欧氏距离通常很大；而如果使用余弦 相似度的话，它们之间的夹角可能很小，因而相似度高。此外，在文本、图像、 视频等领域，研究的对象的特征维度往往很高，余弦相似度在高维情况下依然保 持“相同时为1，正交时为0，相反时为−1”的性质，而欧氏距离的数值则受维度的 影响，范围不固定，并且含义也比较模糊。

### 3.6 A/B测试

AB测试是为Web或App界面或流程制作两个（A/B）或多个（A/B/n）版本，在同一时间维度，分别让组成成分相同（相似）的访客群组（目标人群）随机的访问这些版本，收集各群组的用户体验数据和业务数据，最后分析、评估出最好版本，正式采用。

### 3.7 模型评估方法

1. **Holdout检验**

   Holdout 检验是最简单也是最直接的验证方法，它将原始的样本集合随机划分 成训练集和验证集两部分。比方说，对于一个点击率预测模型，我们把样本按照 70%～30% 的比例分成两部分，70% 的样本用于模型训练；30% 的样本用于模型 验证，包括绘制ROC曲线、计算精确率和召回率等指标来评估模型性能。

   Holdout 检验的缺点很明显，即在验证集上计算出来的最后评估指标与原始分 组有很大关系。为了消除随机性，研究者们引入了“交叉检验”的思想。

2. **交叉检验**

   k-fold交叉验证：首先将全部样本划分成k个大小相等的样本子集；依次遍历 这k个子集，每次把当前子集作为验证集，其余所有子集作为训练集，进行模型的 训练和评估；最后把k次评估指标的平均值作为最终的评估指标。在实际实验 中，k经常取10。

3. **自助法**

   不管是Holdout检验还是交叉检验，都是基于划分训练集和测试集的方法进行 模型评估的。然而，当样本规模比较小时，将样本集进行划分会让训练集进一步 减小，这可能会影响模型训练效果。有没有能维持训练集样本规模的验证方法 呢？自助法可以比较好地解决这个问题。 

   自助法是基于自助采样法的检验方法。对于总数为n的样本集合，进行n次有 放回的随机抽样，得到大小为n的训练集。n次采样过程中，有的样本会被重复采 样，有的样本没有被抽出过，将这些没有被抽出的样本作为验证集，进行模型验 证，这就是自助法的验证过程。

### 3.8 超参数调优

为了进行超参数调优，我们一般会采用网格搜索、随机搜索、贝叶斯优化等 算法。在具体介绍算法之前，需要明确超参数搜索算法一般包括哪几个要素。一 是目标函数，即算法需要最大化/最小化的目标；二是搜索范围，一般通过上限和 下限来确定；三是算法的其他参数，如搜索步长。

- **网格搜索**，可能是最简单、应用最广泛的超参数搜索算法，它通过查找搜索范 围内的所有的点来确定最优值。如果采用较大的搜索范围以及较小的步长，网格 搜索有很大概率找到全局最优值。然而，**这种搜索方案十分消耗计算资源和时间**，特别是需要调优的超参数比较多的时候。因此，在实际应用中，网格搜索法一般会先使用较广的搜索范围和较大的步长，来寻找全局最优值可能的位置；然 后会逐渐缩小搜索范围和步长，来寻找更精确的最优值。这种操作方案可以降低 所需的时间和计算量，但由于目标函数一般是非凸的，所以很可能会错过全局最 优值。
- **随机搜索**，随机搜索的思想与网格搜索比较相似，只是不再测试上界和下界之间的所有 值，而是在搜索范围中随机选取样本点。它的理论依据是，如果样本点集足够 大，那么通过随机采样也能大概率地找到全局最优值，或其近似值。随机搜索一 般会比网格搜索要快一些，但是和网格搜索的快速版一样，它的结果也是没法保证的。
- **贝叶斯优化算法**，贝叶斯优化算法在寻找最优最值参数时，采用了与网格搜索、随机搜索完全 不同的方法。网格搜索和随机搜索在测试一个新点时，会忽略前一个点的信息； 而贝叶斯优化算法则充分利用了之前的信息。贝叶斯优化算法通过对目标函数形 状进行学习，找到使目标函数向全局最优值提升的参数。

### 3.9 过拟合和欠拟合

过拟合是指模型对于训练数据拟合呈过当的情况，反映到评估指标上，就是 模型在训练集上的表现很好，但在测试集和新数据上的表现较差。欠拟合指的是 模型在训练和预测时表现都不好的情况。下图形象地描述了过拟合和欠拟合的区别。

![](http://wx1.sinaimg.cn/mw690/00630Defly1g5e5937nuej30hw05i3zl.jpg)

1. **防止过拟合：**
   - 从数据入手，获得更多的训练数据。
   - 降低模型复杂度。
   - 正则化方法，给模型的参数加上一定的正则约束。
   - 集成学习方法，集成学习是把多个模型集成在一起。
2. **防止欠拟合：**
   - 添加新特征。
   - 增加模型复杂度。
   - 减小正则化系数。

## 4. 检验方法

### 4.1 KS检验

Kolmogorov-Smirnov检验是基于累计分布函数的，用于检验一个分布是否符合某种理论分布或比较两个经验分布是否有显著差异。

- 单样本K-S检验是用来检验一个数据的观测经验分布是否符合已知的理论分布。
- 两样本K-S检验由于对两样本的经验分布函数的位置和形状参数的差异都敏感，所以成为比较两样本的最有用且最常用的非参数方法之一。

检验统计量为：![](https://latex.codecogs.com/gif.latex?D_r=max_x|F_n(x)-F(x)|)

其中  ![](https://latex.codecogs.com/gif.latex?F_n(x))为观察序列值，![](https://latex.codecogs.com/gif.latex?F(x))为理论序列值或另一观察序列值。

### 4.2 T检验

T检验，也称student t检验，主要用户样本含量较小，总体标准差未知的正态分布。

t检验是用t分布理论来推论差异发生的概率，从而比较两个平均数的差异是否显著。

t检验分为单总体检验和双总体检验。

### 4.3 F检验

T检验和F检验的由来：为了确定从样本中的统计结果推论到总体时所犯错的概率。F检验又叫做联合假设检验，也称方差比率检验、方差齐性检验。是由英国统计学家Fisher提出。通过比较两组数据的方差，以确定他们的精密度是否有显著性差异。

### 4.4 Grubbs检验

一组测量数据中，如果个别数据偏离平均值很远，那么称这个数据为“可疑值”。用格拉布斯法判断，能将“可疑值”从测量数据中剔除。

### 4.5 卡方检验

卡方检验就是统计样本的实际观测值与理论推断值之间的偏离程度，实际观测值与理论推断值之间的偏离程度就决定卡方值的大小，卡方值越大，越不符合；卡方值越小，偏差越小，越趋于符合，若两个值完全相等时，卡方值就为0，表明理论值完全符合。

1. 提出原假设H0：总体X的分布函数F(x)；

2. 将总体x的取值范围分成k个互不相交的小区间A1-Ak；

3. 把落入第i个区间Ai的样本的个数记做fi，成为组频数，f1+f2+f3+...+fk = n；

4. 当H0为真时，根据假设的总体理论分布，可算出总体X的值落入第i个小区间Ai的概率pi，于是n*pi就是落入第i个小区间Ai的样本值的理论频数；

5. 当H0为真时，n次试验中样本落入第i个小区间Ai的频率fi/n与概率pi应该很接近。基于这种思想，皮尔逊引入检测统计量：

   ![](https://latex.codecogs.com/gif.latex?x^2=\sum_{i=1}^{k}\frac{(f_i-np_i)^2}{np_i})

   在H0假设成立的情况下服从自由度为k-1的卡方分布。

**KS检验与卡方检验**

**相同点：**都采用实际频数和期望频数只差进行检验

**不同点：**

- 卡方检验主要用于类别数据，而KS检验主要用于有计量单位的连续和定量数据。
- 卡方检验也可以用于定量数据，但必须先将数据分组才能获得实际的观测频数，而KS检验能直接对原始数据进行检验，所以它对数据的利用比较完整。











# 激活函数

**使用非线性激活函数是为了增加神经网络模型的非线性因素**





# 梯度消失和梯度爆炸

目前优化神经网络的方法都是基于BP，即根据损失函数计算的误差通过梯度反向传播的方式，指导深度网络权值的更新优化。其中将误差从末层往前传递的过程需要**链式法则（Chain Rule）**的帮助，因此反向传播算法可以说是梯度下降在链式法则中的应用。

而链式法则是一个**连乘的形式**，所以当层数越深的时候，梯度将以指数形式传播。梯度消失问题和梯度爆炸问题一般随着网络层数的增加会变得越来越明显。在根据损失函数计算的误差通过梯度**反向传播**的方式对深度网络权值进行更新时，得到的**梯度值接近0**或**特别大**，也就是**梯度消失**或**爆炸**。

### 原因

梯度消失：**深层网络**和**采用了不合适的激活函数**

梯度爆炸：**深层网络**和**权值初始化值太大**
（梯度爆炸会伴随一些细微的信号，如：①模型不稳定，导致更新过程中的损失出现显著变化；②训练过程中，在极端情况下，权重的值变得非常大，以至于溢出，导致模型损失变成 NaN等等。）

1. ==深层网络==

对损失函数求不同层的权值偏导，通过链式法则来逐层计算，当层数越深的时候，梯度将指数传播。

如果接近输出层的激活函数求导后梯度值大于1，那么层数增多的时候，最终求出的梯度很容易指数级增长，就会产生**梯度爆炸**；相反，如果小于1，那么经过链式法则的连乘形式，也会很容易衰减至0，就会产生**梯度消失**。

（从深层网络角度来讲，不同的层学习的速度差异很大，表现为网络中靠近输出的层学习的情况很好，靠近输入的层学习的很慢，有时甚至训练了很久，前几层的权值和刚开始随机初始化的值差不多）

2. ==采用了不合适的激活函数==

比如sigmoid。如果使用sigmoid作为激活函数，它的梯度是不超过0.25的，而初始化的网络权值通常也都小于1，经过链式求导，层数越多，求导结果越小，然后就很容易发生**梯度消失**。

（当梯度消失发生时，接近于输出层的隐藏层由于其梯度相对正常，所以权值更新时也就相对正常，但是当越靠近输入层时，由于梯度消失现象，会导致靠近输入层的隐藏层权值更新缓慢或者更新停滞。这就导致在训练时，只等价于后面几层的浅层网络的学习）

3. ==初始化权重的值过大==

权重比较大的时候，根据链式相乘(反向传播)可以知道，前面的网络层比后面的网络层梯度变化更快，很容易发生梯度爆炸的问题。

### 解决办法

梯度消失和梯度爆炸问题都是因为网络太深，网络权值更新不稳定造成的，本质上是因为梯度反向传播中的连乘效应。

**（1） pre-training + fine-tunning**

**无监督逐层训练**，其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在**预训练完成后，再对整个网络进行“微调”**（fine-tunning）。此思想相当于是先寻找局部最优，然后整合起来寻找全局最优，此方法有一定的好处，但是目前应用的不是很多了。

**（2） 梯度剪切：对梯度设定阈值**

设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。可以**防止梯度爆炸**。

**（3） 权重正则化**

权重正则化（weithts regularization），正则化主要是通过对网络权重做正则来限制过拟合。如果发生梯度爆炸，那么权值就会变的非常大，反过来，**通过正则化项来限制权重的大小**，也**可以在一定程度上防止梯度爆炸的发生**。比较常见的是 L1 正则和 L2 正则。

**（4） 选择relu等梯度大部分落在常数上的激活函数**

relu函数的导数在正数部分是恒等于1的，因此在深层网络中使用relu激活函数就不会导致梯度消失和爆炸的问题。

**（5） batch normalization**

BN就是通过**对每一层的输出规范为均值和方差一致**的方法，**消除了权重参数放大缩小带来的影响**，进而解决梯度消失和爆炸的问题，或者可以理解为BN将输出从饱和区拉倒了非饱和区。

**（6） 残差网络的捷径（shortcut）**

**（7） LSTM的“门（gate）”结构**

LSTM的结构设计可以改善RNN中的梯度消失的问题。

LSTM 通过它内部的“门”可以在接下来更新的时候“记住”前几次训练的”残留记忆“。

# 过拟合

# 机器学习常用模型总结

## SVM

## KNN

所谓K近邻算法，即是给定一个训练数据集，对新的输入实例，**在训练数据集中找到与该实例最邻近的K个实例（也就是上面所说的K个邻居），这K个实例的多数属于某个类，就把该输入实例分类到这个类中。**

## K-means

按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

假设簇划分为$(C_1,C_2,...C_k)$，则我们的目标是最小化平方误差E：
$$
E=∑\limits^k_{i=1}∑\limits_{x∈C_i}||x−μ_i||^2_2
$$
其中$μ_i$是簇$C_i$的均值向量，有时也称为质心，表达式为：
$$
μ_i=\frac{1}{|C_i|}∑\limits_{x∈C_i}x
$$
**k为类别数**。

初始的数据集，假设k=2。
先随机选择了2个k类所对应的类别质心，然后分别求样本中所有点到这两个质心的距离，并标记每个样本的类别为和该样本距离最小的质心的类别，
经过计算样本和2个质心的距离，得到了所有样本点的第一轮迭代后的类别。此时我们对我们当前标记为红色和蓝色的点分别求其新的质心
新的红色质心和蓝色质心的位置已经发生了变动。
重复以上过程，即将所有点的类别标记为距离最近的质心的类别并求新的质心。

## Bagging和Boosting

![image-20240523160429935](C:\Users\CZY\AppData\Roaming\Typora\typora-user-images\image-20240523160429935.png)

### 什么是集成学习？

集成学习通过构建并结合多个学习器来完成学习任务。一般分为以下三类：

1. Bagging：训练多个同类型分类器，然后对结果取平均。（个体学习器之间无强依赖关系）
2. Boosting：对弱分类器加强，通过加权来训练。（学习器之间强依赖关系）
3. Stacking：聚合多种分类/回归模型，对输出结果逐层累加优化

典型的bagging模型有随机森林，典型的boosting模型有Adaboost, GBDT和XGBoost。

注意：集成学习中用到的分类器应当“好而不同”，一方面要保证每个分类器不能太拉胯，即要保证一定的准确性（坏）+（坏）=（更坏）；（坏）+（好）!=（好）。另一方面要保证分类器的多样性，即模型之间即使是同类型也要有差异（好1）+（好2）=（更好）

### Bagging模型

就是并行训练一大堆分类器。
典型的Bagging模型有随机森林。

**1. 随机森林原理**

- 随机：二重随机性（数据随机采样+特征随机选择）
- 森林：多个决策树并行放在一起构成森林

![img](https://pic4.zhimg.com/v2-7a09805e4a47f633c275c437bb0a67ef_r.jpg)

**2. 随机森林的步骤**

> 1. 预设模型的超参数（一共几棵树，分几层）,用于构建森林。
> 2. 随机采样，用于后续训练每棵决策树。
> 3. 输入待测样本到每棵树中，得到每棵树的分类结果。
> 4. 再把每棵树的结果整合,可能取众数，也可能取平均，得到最终结果。

**3. 随机森林的优缺点**

|      | 随机森林                                                     |
| ---- | ------------------------------------------------------------ |
| 优点 | 1. 模型随机性强，不容易过拟合 2. 对异常值、outlier不敏感、抗噪音能力强（即使大块儿特征遗失，仍可以保持准确度）3. 处理高维数据时无需降维，处理速度快4. 树状结构，模型可解释性强（易辨别每个特征的重要性差异） |
| 缺点 | 1. 在噪声实在过大的分类和回归问题上已经被证实会过拟合。 2. 模型的起点高，天花板低（三个偏科同学干不过一个全科优等生），因此不擅长处理过于困难的问题。 |

### Boosting模型

boost，顾名思义，就是一种增加、促进的意思。而Boosting模型是一种前人栽树后人乘凉的算法它通过对weak learner中做错的训练样本多加关注并调整，然后生成下一个基学习器，不断迭代，直到学习器的数量达到预先设定的T个时停止，最终将这T个基学习器加权结合。经典Boosting方法有Adaboost、GDBT、Xgboost。

#### 1. Adaboost

**(1) Adaboost原理**

全称为Adaptive Boosting自适应增强算法, 它是一种有监督学习，适用于解决回归和分类问题。

为什么叫自适应呢？因为它不仅分类，还添加了“权重”的概念。前一个基学习器中被分类错误的样本的权重会增大，而被分类正确的样本的权重会减小，然后用它们来训练下一个基学习器。除此之外，在每一轮迭代时都加入一个新的弱学习器。经过多次【分类-加权-添学习器-再分类】迭代，在“达到预期错误率”或者“预定最大迭代次数”后确定最终的强学习器。

**(2) Adaboost步骤**

![img](https://pic4.zhimg.com/80/v2-76115fe64c1632dfdc4adf9473c65a93_720w.webp)

**(3) Adaboost的优缺点**

|      | Adaboost算法                                                 |
| ---- | ------------------------------------------------------------ |
| 优点 | 1. 【最终性能强】Adaboost不像RF一样无脑并联，而是将后者的学习内容建立在前者的短板上。 2.【天花板高】Adaboost不像RF那样对输出结果求均值or众数，而是整合各个learner的输出结果，并在其基础上增加了权重。 |
| 缺点 | 1. 【性能起点低】对异常值、outlier很敏感 2.【计算速度慢】因为是后者需要不断改进前者，这种特殊的结果导致后者与前者无法并行计算。 |

#### 2. GBDT模型

GBDT-Gradient Boosting Decision Tree梯度提升决策树，有很多简称，有GBT（Gradient Boosting Tree）, GTB（Gradient Tree Boosting ）， GBRT（Gradient Boosting Regression Tree）, MART(Multiple Additive Regression Tree)，其实都是指的同一种算法，它常用于对一些数值型结果进行预测，不常用作分类。GBDT是GBM(Gradient Boosting Machine）的一种，理论上GBM可以选多种不同的算法作为基学习器。

> **为什么GBDT要用决策树作基学习器呢？**DT是If-then规则的集合，易于理解，可解释性强，运算速度快，可以做更少的特征工程，而且不用担心特征之间相互依赖 （DT可以自动组合多个特征），而且CART决策树既可以做分类，也可以做回归。
> **为什么要对DT做回归？**因为GBDT需要累加所有树的结果，而这种累加是无法通过离散样本分类完成的，因此GBDT需要决策树中能够搞定回归任务的CART树去执行。注意：这里用到的CART树是回归树，而不是分类树（尽管GBDT在调整后可以用作分类，但是并不代表它的树模型用的是分类树）。

**(1) GBDT的原理**

GBDT的中心思想就是**对残差进行拟合，使得残差不断减小**，最终取得较为准确的预测结果。

残差（residual）是什么？残差 = 观察到的真实值 - 预测值or拟合值

例如对一个年龄进行预测：

- 真实值30岁，预测值20岁，计算残差为30-20=10岁
- 残差10岁，预测这个差值是6岁，计算残差为10-6=4岁
- 残差4岁，预测这个差值是3岁，计算残差为4-3=1岁
- 最终，预测值与真实值之间的差值通过“拟合残差”的方法缩小到了1岁。

![img](https://pic3.zhimg.com/80/v2-cc640d4afe251607a71eca463b26ae3e_720w.webp)

当然，为了防止每个学习器的拟合能力过强（过拟合），提前给定一个缩放系数v来控制F(X).

**(2) GBDT的步骤**

*这个由于涉及到太多数学公式，看得有点迷TAT, 暂时搁置，后续再回来补充。*

有兴趣的可以参考这个 [GBDT原理&流程](https://link.zhihu.com/?target=https%3A//blog.csdn.net/shine19930820/article/details/65633436)

**(3) GBDT的优缺点**

|      | GBDT模型                                                     |
| ---- | ------------------------------------------------------------ |
| 优点 | 1. 能灵活处理各种类型数据（包括离散值和连续值） 2. 在有限的调参时间下，预测的准确度较高 |
| 缺点 | 1. 由于weak learner 之间存在依赖关系，难以并行 训练数据。 2. 数据维度高的时候，计算复杂度会爆炸。 |

#### **3. XGBoost**

**(1) XGBoost的基本原理**

XGBoost也就是Exterme Gradient Boosting（极限梯度提升），作为Kaggle比赛的神器，它是GBDT改进而来的，是一个加法模型，基模型一般选择树模型（但也可以选择其他模型：例如逻辑回归）。XGBoost通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差基础上进行训练。对弱分类器的要求一般是足够简单，并且是低方差和高偏差的。因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 弱分类器一般会选择为CART TREE（也就是分类回归树，同时XGBoost还支持线性分类器）。由于上述高偏差和简单的要求 每个分类回归树的深度不会很深。最终的总分类器 是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）[[1\]](https://zhuanlan.zhihu.com/p/361725445#ref_1)。因此，XGBoost模型目标函数为(1)，它由损失函数和正则化项两部分组成：

![img](https://pic4.zhimg.com/80/v2-01926563d269b3207b8e88ef2927e773_720w.webp)

对于等式(1)， 它由加号前的“损失函数”和加号后的“正则化项”两部分组成。

> **损失函数部分：**
> 𝑦𝑖是真实样本标签，𝑦^𝑖是第𝑖个样本𝑥𝑖的预测值：𝑦^𝑖=∑𝑘=1𝐾𝑓𝑘(𝑥𝑖),𝑓𝑘∈𝐹；
> **正则化部分：**
> 𝑓𝑘是第𝑘个树模型函数，一共由𝐾棵，𝛾是树个数的惩罚正则项，具有剪枝作用，𝜆为叶子权重惩罚正则项，防止过拟合，𝜔为叶子节点的权重向量[[2\]](https://zhuanlan.zhihu.com/p/361725445#ref_2)。

对于等式(2)，它由“叶子节点的数量”和“叶子节点权重向量的L2范数”组成，

> 代表着树的复杂度，其中T为叶子节点的个数，||w||为叶子节点向量的模 。γ表示节点切分的难度，λ表示L2正则化系数。

**(2) XGBoost模型优化提升的原因（优点）**

- 【第一点：适用广】Xgboost与GBDT区别在于后者只能用CART决策树，而XGBoost额外支持线性分类器（线性回归&逻辑回归）,提升了模型的应用范围。
- 【第二点：正则项】XGBoost在损失函数中加入了正则项，模型的复杂度可以得到控制。从权衡方差&偏差的角度，正则化的加入，使得使得方差降低，也就意味着训练出来的模型更加简单。可以防止过拟合，以及提升了泛化能力。
- 【第三点：二阶导】相比于GBDT的模型优化（用到了损失函数一阶导），XGBoost使用了二阶泰勒展开，进而得到了一阶导和二阶导，提升了模型优化速度。
- 【第四点：抽样】XGBoost借鉴随机森林模型，支持列抽样&行抽样，防止过拟合，且降低了计算量。
- 【第五点：缺失值处理】XGBoost能够自动处理缺失值，并将其单独作为一个分支。

**(3) XGBoost的并行操作**

这个并行不是指多颗决策树模型同时运行（XGBoost也是得等上一批迭代完之后才能进行下一批），此并行指的是在特征上并行，XGBoost在训练前会预先对数据排序，并将其保存为block模块，后续直接复用模块，计算量降低。所以此并行指的是块状结构并行。

### 集成学习面试题

**Q: 强学习器与弱学习器划分依据？**

没有明确界限，都是相对的，比如决策树与RF相比，后者就是强学习器。因此可以通过对复杂场景的处理能力进行判断。多个弱学习器可以构成强学习器，即为模型集成。

**Q: 模型集成与模型融合的概念？**

- 模型集成：用弱学习器构建强学习器，可同质也可异质。（例如Bagging中的RF,Boosting中的GBDT）
- 模型融合: 多个学习器的结果组合在一起进行平均或者投票，获得最终结果。（预测问题——平均法；分类问题——投票法）

**Q: 随机森林与Boosting模型之间的区别？**

- 随机森林中各个决策树模型相互独立
- Boosting模型的新一代需要依赖于上一代模型的结果，并不相互独立。

**Q: 基于决策树的Boosting模型有哪些？原理是什么？**

- Adaboost和GBDT
- Adaboost会给前一代中判错和判对的样本分配不同的权重，下一代会重点关照判错对。
- 而GBDT则是通过残差拟合的方式，缩小真实值与预测值之间的差距。它们俩都能处理连续or离散变量；不需太多事前假设；可处理复杂问题。

**Q: Adaboost、GBDT和XGBoost的区别？**

1. 原理不同：Adaboost是加法模型，使用的是指数损失函数，它在累加时对判错的样本赋予权重来改进模型；GBDT和XGBoost都是通过拟合残差的方式改进模型。
2. 基学习器范围不同：Adaboost和GBDT默认都是用决策树，而XGBoost不仅可以用决策树，也可以用线性模型（线性回归&逻辑回归）
3. 求导不同：XGBoost是泰勒展开二项式逼近，可以二阶求导，收敛速度更快，而GBDT只能一阶求导。
4. 正则项方面：三者中只有XGBoost引入了正则项约束，而GBDT和Adaboost没有。
5. 处理方式上：XGBoost是以模块化并行处理，Adaboost则是逐步累加，需要等。

**Q: Adaboost、GBDT和XGBoost的联系？**

他们都是Boosting模型，种前人栽树后人乘凉，它们都通过对weak learner中做错的训练样本多加关注并调整，然后生成下一个基学习器，不断迭代。





















## 随机森林

### 1.什么是随机森林

#### 1.1 Bagging思想

Bagging是bootstrap aggregating。思想就是从总体样本当中随机取一部分样本进行训练，通过多次这样的结果，进行投票获取平均值作为结果输出，这就极大可能的避免了不好的样本数据，从而提高准确度。因为有些是不好的样本，相当于噪声，模型学入噪声后会使准确度不高。

**举个例子**：

假设有1000个样本，如果按照以前的思维，是直接把这1000个样本拿来训练，但现在不一样，先抽取800个样本来进行训练，假如噪声点是这800个样本以外的样本点，就很有效的避开了。重复以上操作，提高模型输出的平均值。

#### 1.2 随机森林

Random Forest(随机森林)是一种基于树模型的Bagging的优化版本，一棵树的生成肯定还是不如多棵树，因此就有了随机森林，解决决策树泛化能力弱的特点。(可以理解成三个臭皮匠顶过诸葛亮)

而同一批数据，用同样的算法只能产生一棵树，这时Bagging策略可以帮助我们产生不同的数据集。**Bagging**策略来源于bootstrap aggregation：从样本集（假设样本集N个数据点）中重采样选出Nb个样本（有放回的采样，样本数据点个数仍然不变为N），在所有样本上，对这n个样本建立分类器（ID3\C4.5\CART\SVM\LOGISTIC），重复以上两步m次，获得m个分类器，最后根据这m个分类器的投票结果，决定数据属于哪一类。

**每棵树的按照如下规则生成：**

1. 如果训练集大小为N，对于每棵树而言，**随机**且有放回地从训练集中的抽取N个训练样本，作为该树的训练集；
2. 如果每个样本的特征维度为M，指定一个常数m<<M，**随机**地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的；
3. 每棵树都尽最大程度的生长，并且没有剪枝过程。

一开始我们提到的随机森林中的“随机”就是指的这里的两个随机性。两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。

总的来说就是随机选择样本数，随机选取特征，随机选择分类器，建立多颗这样的决策树，然后通过这几课决策树来投票，决定数据属于哪一类(**投票机制有一票否决制、少数服从多数、加权多数**)

### 2. 随机森林分类效果的影响因素

- 森林中任意两棵树的相关性：相关性越大，错误率越大；
- 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。

减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。

### 3. 随机森林有什么优缺点

**优点：**

- 在当前的很多数据集上，相对其他算法有着很大的优势，表现良好。
- 它能够处理很高维度（feature很多）的数据，并且不用做特征选择(因为特征子集是随机选择的)。
- 在训练完后，它能够给出哪些feature比较重要。
- 训练速度快，容易做成并行化方法(训练时树与树之间是相互独立的)。
- 在训练过程中，能够检测到feature间的互相影响。
- 对于不平衡的数据集来说，它可以平衡误差。
- 如果有很大一部分的特征遗失，仍可以维持准确度。

**缺点：**

- 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟合。
- 对于有不同取值的属性的数据，取值划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产出的属性权值是不可信的。

### 4. 随机森林如何处理缺失值？

根据随机森林创建和训练的特点，随机森林对缺失值的处理还是比较特殊的。

- 首先，给缺失值预设一些估计值，比如数值型特征，选择其余数据的中位数或众数作为当前的估计值
- 然后，根据估计的数值，建立随机森林，把所有的数据放进随机森林里面跑一遍。记录每一组数据在决策树中一步一步分类的路径.
- 判断哪组数据和缺失数据路径最相似，引入一个相似度矩阵，来记录数据之间的相似度，比如有N组数据，相似度矩阵大小就是N*N
- 如果缺失值是类别变量，通过权重投票得到新估计值，如果是数值型变量，通过加权平均得到新的估计值，如此迭代，直到得到稳定的估计值。

其实，该缺失值填补过程类似于推荐系统中采用协同过滤进行评分预测，先计算缺失特征与其他特征的相似度，再加权得到缺失值的估计，而随机森林中计算相似度的方法（数据在决策树中一步一步分类的路径）乃其独特之处。

### 5. 什么是OOB？随机森林中OOB是如何计算的，它有什么优缺点？

**OOB**：

上面我们提到，构建随机森林的关键问题就是如何选择最优的m，要解决这个问题主要依据计算袋外错误率oob error（out-of-bag error）。

bagging方法中Bootstrap每次约有1/3的样本不会出现在Bootstrap所采集的样本集合中，当然也就没有参加决策树的建立，把这1/3的数据称为**袋外数据oob（out of bag）**,它可以用于取代测试集误差估计方法。

**袋外数据(oob)误差的计算方法如下：**

- 对于已经生成的随机森林,用袋外数据测试其性能,假设袋外数据总数为O,用这O个袋外数据作为输入,带进之前已经生成的随机森林分类器,分类器会给出O个数据相应的分类
- 因为这O条数据的类型是已知的,则用正确的分类与随机森林分类器的结果进行比较,统计随机森林分类器分类错误的数目,设为X,则袋外数据误差大小=X/O

**优缺点**：

这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。 

### 6. 随机森林的过拟合问题

1. 你已经建了一个有10000棵树的随机森林模型。在得到0.00的训练误差后，你非常高兴。但是，验证错误是34.23。到底是怎么回事？你还没有训练好你的模型吗？

   答：该模型过度拟合，因此，为了避免这些情况，我们要用交叉验证来调整树的数量。

## 决策树

- [1. 什么是决策树](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#1-什么是决策树)
  - [1.1 决策树的基本思想](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#11-决策树的基本思想)
  - [1.2 “树”的成长过程](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#12-树的成长过程)
  - [1.3 "树"怎么长](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#13-树怎么长)
  - [1.3.1 ID3算法](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#131-id3算法)
  - [1.3.2 C4.5](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#132-c45)
  - [1.3.3 CART算法](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#133-cart算法)
  - [1.3.4 三种不同的决策树](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#134-三种不同的决策树)
- [2. 树形结构为什么不需要归一化?](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#2-树形结构为什么不需要归一化)
- [3. 分类决策树和回归决策树的区别](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#3-分类决策树和回归决策树的区别)
- [4. 决策树如何剪枝](https://github.com/NLP-LOVE/ML-NLP/tree/master/Machine%20Learning/3.Desition%20Tree#4-决策树如何剪枝)
- [5. 代码实现](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.Desition%20Tree/DecisionTree.ipynb)

### 1. 什么是决策树

#### 1.1 决策树的基本思想

其实用一下图片能更好的理解LR模型和决策树模型算法的根本区别，我们可以思考一下一个决策问题：是否去相亲，一个女孩的母亲要给这个女海介绍对象。

![image](https://wx2.sinaimg.cn/large/00630Defly1g4q286viibj30pk0pfk09.jpg)

大家都看得很明白了吧！LR模型是一股脑儿的把所有特征塞入学习，而决策树更像是编程语言中的if-else一样，去做条件判断，这就是根本性的区别。

#### 1.2 “树”的成长过程

决策树基于“树”结构进行决策的，这时我们就要面临两个问题 ：

- “树”怎么长。
- 这颗“树”长到什么时候停。

弄懂了这两个问题，那么这个模型就已经建立起来了，决策树的总体流程是“分而治之”的思想，一是自根至叶的递归过程，一是在每个中间节点寻找一个“划分”属性，相当于就是一个特征属性了。接下来我们来逐个解决以上两个问题。

##### 这颗“树”长到什么时候停

- 当前结点包含的样本全属于同一类别，无需划分；例如：样本当中都是决定去相亲的，属于同一类别，就是不管特征如何改变都不会影响结果，这种就不需要划分了。
- 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分；例如：所有的样本特征都是一样的，就造成无法划分了，训练集太单一。
- 当前结点包含的样本集合为空，不能划分。

#### 1.3 "树"怎么长

在生活当中，我们都会碰到很多需要做出决策的地方，例如：吃饭地点、数码产品购买、旅游地区等，你会发现在这些选择当中都是依赖于大部分人做出的选择，也就是跟随大众的选择。其实在决策树当中也是一样的，当大部分的样本都是同一类的时候，那么就已经做出了决策。

我们可以把大众的选择抽象化，这就引入了一个概念就是纯度，想想也是如此，大众选择就意味着纯度越高。好，在深入一点，就涉及到一句话：**信息熵越低，纯度越高**。我相信大家或多或少都听说过“熵”这个概念，信息熵通俗来说就是用来度量包含的“信息量”，如果样本的属性都是一样的，就会让人觉得这包含的信息很单一，没有差异化，相反样本的属性都不一样，那么包含的信息量就很多了。

一到这里就头疼了，因为马上要引入信息熵的公式，其实也很简单：

![](https://latex.codecogs.com/gif.latex?Ent(D)=-\sum_{k=1}^{|y|}p_klog_2p_k)

Pk表示的是：当前样本集合D中第k类样本所占的比例为Pk。

**信息增益**

废话不多说直接上公式：

![image](https://wx3.sinaimg.cn/large/00630Defly1g4q5h6oby7j30he08tdh5.jpg)

看不懂的先不管，简单一句话就是：划分前的信息熵--划分后的信息熵。表示的是向纯度方向迈出的“步长”。

好了，有了前面的知识，我们就可以开始“树”的生长了。

##### 1.3.1 ID3算法

解释：在根节点处计算信息熵，然后根据属性依次划分并计算其节点的信息熵，用根节点信息熵--属性节点的信息熵=信息增益，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。

如果不理解的，可以查看我分享的图片示例，结合我说的，包你看懂：

1. [第一张图.jpg](https://www.wailian.work/images/2018/12/11/image39e7b.png)
2. [第二张图.jpg](https://www.wailian.work/images/2018/12/11/image61cdc.png)
3. [第三张图.jpg](https://www.wailian.work/images/2018/12/11/image9e194.png)
4. [第四张图.jpg](https://www.wailian.work/images/2018/12/11/image09288.png)

不过，信息增益有一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个 算法C4.5。

##### 1.3.2 C4.5

为了解决信息增益的问题，引入一个信息增益率：

![](https://latex.codecogs.com/gif.latex?Gain\_ratio(D,a)=\frac{Gain(D,a)}{IV(a)})

其中：

![](https://latex.codecogs.com/gif.latex?IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|})

属性a的可能取值数目越多(即V越大)，则IV(a)的值通常就越大。**信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**不过有一个缺点：

- 缺点：信息增益率偏向取值较少的特征。

使用信息增益率：基于以上缺点，并不是直接选择信息增益率最大的特征，而是现在候选特征中找出信息增益高于平均水平的特征，然后在这些特征中再选择信息增益率最高的特征。

##### 1.3.3 CART算法

数学家真实聪明，想到了另外一个表示纯度的方法，叫做基尼指数(讨厌的公式)：

![image](https://wx1.sinaimg.cn/large/00630Defly1g4q5dmvyykj30eb01edfs.jpg)

表示在样本集合中一个随机选中的样本被分错的概率。举例来说，现在一个袋子里有3种颜色的球若干个，伸手进去掏出2个球，颜色不一样的概率，这下明白了吧。**Gini(D)越小，数据集D的纯度越高。**

###### 举个例子

假设现在有特征 “学历”，此特征有三个特征取值： “本科”，“硕士”， “博士”，

当使用“学历”这个特征对样本集合D进行划分时，划分值分别有三个，因而有三种划分的可能集合，划分后的子集如下：

1.划分点： “本科”，划分后的子集合 ： {本科}，{硕士，博士}

2.划分点： “硕士”，划分后的子集合 ： {硕士}，{本科，博士}

3.划分点： “硕士”，划分后的子集合 ： {博士}，{本科，硕士}}

对于上述的每一种划分，都可以计算出基于 **划分特征= 某个特征值** 将样本集合D划分为两个子集的纯度：

![](https://latex.codecogs.com/gif.latex?Gini(D,A)=\frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{|D|}Gini(D_2))

因而**对于一个具有多个取值（超过2个）的特征，需要计算以每一个取值作为划分点，对样本D划分之后子集的纯度Gini(D,Ai)，(其中Ai 表示特征A的可能取值)**

然后从所有的可能划分的Gini(D,Ai)中找出Gini指数最小的划分，这个划分的划分点，便是使用特征A对样本集合D进行划分的最佳划分点。到此就可以长成一棵“大树”了。

##### 1.3.4 三种不同的决策树

- **ID3**：取值多的属性，更容易使数据更纯，其信息增益更大。

  训练得到的是一棵庞大且深度浅的树：不合理。

- **C4.5**：采用信息增益率替代信息增益。

- **CART**：以基尼系数替代熵，最小化不纯度，而不是最大化信息增益。

### 2. 树形结构为什么不需要归一化?

因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。
按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。

对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。
但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。

### 3. 分类决策树和回归决策树的区别

Classification And Regression Tree(CART)是决策树的一种，CART算法既可以用于创建分类树（Classification Tree），也可以用于创建回归树（Regression Tree），两者在建树的过程稍有差异。

**回归树**：

CART回归树是假设树为二叉树，通过不断将特征进行分裂。比如当前树结点是基于第j个特征值进行分裂的，设该特征值小于s的样本划分为左子树，大于s的样本划分为右子树。 

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415343854853617715.png)

而CART回归树实质上就是在该特征维度对样本空间进行划分，而这种空间划分的优化是一种NP难问题，因此，在决策树模型中是使用启发式方法解决。典型CART回归树产生的目标函数为：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438551488112806.png)

因此，当我们为了求解最优的切分特征j和最优的切分点s，就转化为求解这么一个目标函数：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415343855213970444.png)

所以我们只要遍历所有特征的的所有切分点，就能找到最优的切分特征和切分点。最终得到一棵回归树。

参考文章：[经典算法详解--CART分类决策树、回归树和模型树](https://blog.csdn.net/jiede1/article/details/76034328)

### 4. 决策树如何剪枝

决策树的剪枝基本策略有 预剪枝 (Pre-Pruning) 和 后剪枝 (Post-Pruning)。

- **预剪枝**：其中的核心思想就是，在每一次实际对结点进行进一步划分之前，先采用验证集的数据来验证如果划分是否能提高划分的准确性。如果不能，就把结点标记为叶结点并退出进一步划分；如果可以就继续递归生成节点。
- **后剪枝**：后剪枝则是先从训练集生成一颗完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来泛化性能提升，则将该子树替换为叶结点。



## GBDT

### 1. 解释一下GBDT算法的过程

GBDT(Gradient Boosting Decision Tree)，全名叫梯度提升决策树，使用的是**Boosting**的思想。

#### 1.1 Boosting思想

Boosting方法训练基分类器时采用串行的方式，各个基分类器之间有依赖。它的基本思路是将基分类器层层叠加，每一层在训练的时候，对前一层基分类器分错的样本，给予更高的权重。测试时，根据各层分类器的结果的加权得到最终结果。 

Bagging与Boosting的串行训练方式不同，Bagging方法在训练过程中，各基分类器之间无强依赖，可以进行并行训练。

#### 1.2 GBDT原来是这么回事

GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的残差(这个残差就是预测值与真实值之间的误差)。当然了，它里面的弱分类器的表现形式就是各棵树。

举一个非常简单的例子，比如我今年30岁了，但计算机或者模型GBDT并不知道我今年多少岁，那GBDT咋办呢？

- 它会在第一个弱分类器（或第一棵树中）随便用一个年龄比如20岁来拟合，然后发现误差有10岁；
- 接下来在第二棵树中，用6岁去拟合剩下的损失，发现差距还有4岁；
- 接着在第三棵树中用3岁拟合剩下的差距，发现差距只有1岁了；
- 最后在第四课树中用1岁拟合剩下的残差，完美。
- 最终，四棵树的结论加起来，就是真实年龄30岁（实际工程中，gbdt是计算负梯度，用负梯度近似残差）。

**为何gbdt可以用用负梯度近似残差呢？**

回归任务下，GBDT 在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155214962034944638.gif)

那此时的负梯度是这样计算的

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155214962416670973.gif)

所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155214963633267938.gif)，即“当前预测模型的值”，也就是对它求负梯度。

**训练过程**

简单起见，假定训练集只有4个人：A,B,C,D，他们的年龄分别是14,16,24,26。其中A、B分别是高一和高三学生；C,D分别是应届毕业生和工作两年的员工。如果是用一棵传统的回归决策树来训练，会得到如下图所示结果：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438568191303958.png)

现在我们使用GBDT来做这件事，由于数据太少，我们限定叶子节点做多有两个，即每棵树都只有一个分枝，并且限定只学两棵树。我们会得到如下图所示结果：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438570529256895.png)

在第一棵树分枝和图1一样，由于A,B年龄较为相近，C,D年龄较为相近，他们被分为左右两拨，每拨用平均年龄作为预测值。

- 此时计算残差（残差的意思就是：A的实际值 - A的预测值 = A的残差），所以A的残差就是实际值14 - 预测值15 = 残差值-1。
- 注意，A的预测值是指前面所有树累加的和，这里前面只有一棵树所以直接是15，如果还有树则需要都累加起来作为A的预测值。

然后拿它们的残差-1、1、-1、1代替A B C D的原值，到第二棵树去学习，第二棵树只有两个值1和-1，直接分成两个节点，即A和C分在左边，B和D分在右边，经过计算（比如A，实际值-1 - 预测值-1 = 残差0，比如C，实际值-1 - 预测值-1 = 0），此时所有人的残差都是0。残差值都为0，相当于第二棵树的预测值和它们的实际值相等，则只需把第二棵树的结论累加到第一棵树上就能得到真实年龄了，即每个人都得到了真实的预测值。

换句话说，现在A,B,C,D的预测值都和真实年龄一致了。Perfect！

- A: 14岁高一学生，购物较少，经常问学长问题，预测年龄A = 15 – 1 = 14
- B: 16岁高三学生，购物较少，经常被学弟问问题，预测年龄B = 15 + 1 = 16
- C: 24岁应届毕业生，购物较多，经常问师兄问题，预测年龄C = 25 – 1 = 24
- D: 26岁工作两年员工，购物较多，经常被师弟问问题，预测年龄D = 25 + 1 = 26

所以，GBDT需要将多棵树的得分累加得到最终的预测得分，且每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差。

### 2. 梯度提升和梯度下降的区别和联系是什么？ 

下表是梯度提升算法和梯度下降算法的对比情况。可以发现，两者都是在每 一轮迭代中，利用损失函数相对于模型的负梯度方向的信息来对当前模型进行更 新，只不过在梯度下降中，模型是以参数化形式表示，从而模型的更新等价于参 数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函 数空间中，从而大大扩展了可以使用的模型种类。

![](http://wx3.sinaimg.cn/mw690/00630Defgy1g4tdwhqzsdj30rp0afdho.jpg)

### 3. **GBDT**的优点和局限性有哪些？ 

#### 3.1 优点

1. 预测阶段的计算速度快，树与树之间可并行化计算。
2. 在分布稠密的数据集上，泛化能力和表达能力都很好，这使得GBDT在Kaggle的众多竞赛中，经常名列榜首。 
3. 采用决策树作为弱分类器使得GBDT模型具有较好的解释性和鲁棒性，能够自动发现特征间的高阶关系。

#### 3.2 局限性

1. GBDT在高维稀疏的数据集上，表现不如支持向量机或者神经网络。
2. GBDT在处理文本分类特征问题上，相对其他模型的优势不如它在处理数值特征时明显。 
3. 训练过程需要串行训练，只能在决策树内部采用一些局部并行的手段提高训练速度。 

### 4. RF(随机森林)与GBDT之间的区别与联系

**相同点**：

- 都是由多棵树组成，最终的结果都是由多棵树一起决定。
- RF和GBDT在使用CART树时，可以是分类树或者回归树。

**不同点**：

- 组成随机森林的树可以并行生成，而GBDT是串行生成
- 随机森林的结果是多数表决表决的，而GBDT则是多棵树累加之和
- 随机森林对异常值不敏感，而GBDT对异常值比较敏感
- 随机森林是减少模型的方差，而GBDT是减少模型的偏差
- 随机森林不需要进行特征归一化。而GBDT则需要进行特征归一化

## XGBoost

### 1. 什么是XGBoost

XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进，被广泛应用在Kaggle竞赛及其他许多机器学习竞赛中并取得了不错的成绩。

说到XGBoost，不得不提GBDT(Gradient Boosting Decision Tree)。因为XGBoost本质上还是一个GBDT，但是力争把速度和效率发挥到极致，所以叫X (Extreme) GBoosted。包括前面说过，两者都是boosting方法。

关于GBDT，这里不再提，可以查看我前一篇的介绍，[点此跳转](https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.2%20GBDT/3.2%20GBDT.md)。

#### 1.1 XGBoost树的定义

先来举个**例子**，我们要预测一家人对电子游戏的喜好程度，考虑到年轻和年老相比，年轻更可能喜欢电子游戏，以及男性和女性相比，男性更喜欢电子游戏，故先根据年龄大小区分小孩和大人，然后再通过性别区分开是男是女，逐一给各人在电子游戏喜好程度上打分，如下图所示。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438577232516800.png)

就这样，训练出了2棵树tree1和tree2，类似之前gbdt的原理，两棵树的结论累加起来便是最终的结论，所以小孩的预测分数就是两棵树中小孩所落到的结点的分数相加：2 + 0.9 = 2.9。爷爷的预测分数同理：-1 + （-0.9）= -1.9。具体如下图所示：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438578739198433.png)

恩，你可能要拍案而起了，惊呼，这不是跟上文介绍的GBDT乃异曲同工么？

事实上，如果不考虑工程实现、解决问题上的一些差异，XGBoost与GBDT比较大的不同就是目标函数的定义。XGBoost的目标函数如下图所示：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438580139159593.png)

其中：

- 红色箭头所指向的L 即为损失函数（比如平方损失函数：![](https://latex.codecogs.com/gif.latex?l(y_i,y^i)=(y_i-y^i)^2))
- 红色方框所框起来的是正则项（包括L1正则、L2正则）
- 红色圆圈所圈起来的为常数项
- 对于f(x)，XGBoost利用泰勒展开三项，做一个近似。**f(x)表示的是其中一颗回归树。**

看到这里可能有些读者会头晕了，这么多公式，**我在这里只做一个简要式的讲解，具体的算法细节和公式求解请查看这篇博文，讲得很仔细**：[通俗理解kaggle比赛大杀器xgboost](https://blog.csdn.net/v_JULY_v/article/details/81410574)

XGBoost的**核心算法思想**不难，基本就是：

1. 不断地添加树，不断地进行特征分裂来生长一棵树，每次添加一个树，其实是学习一个新函数**f(x)**，去拟合上次预测的残差。
2. 当我们训练完成得到k棵树，我们要预测一个样本的分数，其实就是根据这个样本的特征，在每棵树中会落到对应的一个叶子节点，每个叶子节点就对应一个分数
3. 最后只需要将每棵树对应的分数加起来就是该样本的预测值。

显然，我们的目标是要使得树群的预测值![](https://latex.codecogs.com/gif.latex?y_i^{'})尽量接近真实值![](https://latex.codecogs.com/gif.latex?y_i)，而且有尽量大的泛化能力。类似之前GBDT的套路，XGBoost也是需要将多棵树的得分累加得到最终的预测得分（每一次迭代，都在现有树的基础上，增加一棵树去拟合前面树的预测结果与真实值之间的残差）。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438657261833493.png)

那接下来，我们如何选择每一轮加入什么 f 呢？答案是非常直接的，选取一个 f 来使得我们的目标函数尽量最大地降低。这里 f 可以使用泰勒展开公式近似。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase6415343865867530120.png)

实质是把样本分配到叶子结点会对应一个obj，优化过程就是obj优化。也就是分裂节点到叶子不同的组合，不同的组合对应不同obj，所有的优化围绕这个思想展开。到目前为止我们讨论了目标函数中的第一个部分：训练误差。接下来我们讨论目标函数的第二个部分：正则项，即如何定义树的复杂度。

#### 1.2 正则项：树的复杂度

XGBoost对树的复杂度包含了两个部分：

- 一个是树里面叶子节点的个数T
- 一个是树上叶子节点的得分w的L2模平方（对w进行L2正则化，相当于针对每个叶结点的得分增加L2平滑，目的是为了避免过拟合）

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64153438674199471483.png)

我们再来看一下XGBoost的目标函数（损失函数揭示训练误差 + 正则化定义复杂度）：

![](https://latex.codecogs.com/gif.latex?L(\phi)=\sum_{i}l(y_i^{'}-y_i)+\sum_k\Omega(f_t))

正则化公式也就是目标函数的后半部分，对于上式而言，![](https://latex.codecogs.com/gif.latex?y_i^{'})是整个累加模型的输出，正则化项∑kΩ(ft)是则表示树的复杂度的函数，值越小复杂度越低，泛化能力越强。

#### 1.3 树该怎么长

很有意思的一个事是，我们从头到尾了解了xgboost如何优化、如何计算，但树到底长啥样，我们却一直没看到。很显然，一棵树的生成是由一个节点一分为二，然后不断分裂最终形成为整棵树。那么树怎么分裂的就成为了接下来我们要探讨的关键。对于一个叶子节点如何进行分裂，XGBoost作者在其原始论文中给出了一种分裂节点的方法：**枚举所有不同树结构的贪心法**

不断地枚举不同树的结构，然后利用打分函数来寻找出一个最优结构的树，接着加入到模型中，不断重复这样的操作。这个寻找的过程使用的就是**贪心算法**。选择一个feature分裂，计算loss function最小值，然后再选一个feature分裂，又得到一个loss function最小值，你枚举完，找一个效果最好的，把树给分裂，就得到了小树苗。

总而言之，XGBoost使用了和CART回归树一样的想法，利用贪婪算法，遍历所有特征的所有特征划分点，不同的是使用的目标函数不一样。具体做法就是分裂后的目标函数值比单子叶子节点的目标函数的增益，同时为了限制树生长过深，还加了个阈值，只有当增益大于该阈值才进行分裂。从而继续分裂，形成一棵树，再形成一棵树，**每次在上一次的预测基础上取最优进一步分裂/建树。**

#### 1.4 如何停止树的循环生成

凡是这种循环迭代的方式必定有停止条件，什么时候停止呢？简言之，设置树的最大深度、当样本权重和小于设定阈值时停止生长以防止过拟合。具体而言，则

1. 当引入的分裂带来的增益小于设定阀值的时候，我们可以忽略掉这个分裂，所以并不是每一次分裂loss function整体都会增加的，有点预剪枝的意思，阈值参数为（即正则项里叶子节点数T的系数）；
2. 当树达到最大深度时则停止建立决策树，设置一个超参数max_depth，避免树太深导致学习局部样本，从而过拟合；
3. 样本权重和小于设定阈值时则停止建树。什么意思呢，即涉及到一个超参数-最小的样本权重和min_child_weight，和GBM的 min_child_leaf 参数类似，但不完全一样。大意就是一个叶子节点样本太少了，也终止同样是防止过拟合；

### 2. XGBoost与GBDT有什么不同

除了算法上与传统的GBDT有一些不同外，XGBoost还在工程实现上做了大量的优化。总的来说，两者之间的区别和联系可以总结成以下几个方面。 

1. GBDT是机器学习算法，XGBoost是该算法的工程实现。
2. 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模 型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
3. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代 价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
4. 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类 器，比如线性分类器。
5. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机 森林相似的策略，支持对数据进行采样。
6. 传统的GBDT没有设计对缺失值进行处理，XGBoost能够自动学习出缺 失值的处理策略。

### 3. 为什么XGBoost要用泰勒展开，优势在哪里？

XGBoost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得函数做自变量的二阶导数形式, 可以在不选定损失函数具体形式的情况下, 仅仅依靠输入数据的值就可以进行叶子分裂优化计算, 本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了XGBoost的适用性, 使得它按需选取损失函数, 可以用于分类, 也可以用于回归。



## LightGBM

### 1. LightGBM是什么东东

不久前微软DMTK(分布式机器学习工具包)团队在GitHub上开源了性能超越其他boosting工具的LightGBM，在三天之内GitHub上被star了1000次，fork了200次。知乎上有近千人关注“如何看待微软开源的LightGBM？”问题，被评价为“速度惊人”，“非常有启发”，“支持分布式”，“代码清晰易懂”，“占用内存小”等。

LightGBM （Light Gradient Boosting Machine）(请点击[https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM))是一个实现GBDT算法的框架，支持高效率的并行训练。

LightGBM在Higgs数据集上LightGBM比XGBoost快将近10倍，内存占用率大约为XGBoost的1/6，并且准确率也有提升。GBDT在每一次迭代的时候，都需要遍历整个训练数据多次。如果把整个训练数据装进内存则会限制训练数据的大小；如果不装进内存，反复地读写训练数据又会消耗非常大的时间。尤其面对工业级海量的数据，普通的GBDT算法是不能满足其需求的。

LightGBM提出的主要原因就是为了解决GBDT在海量数据遇到的问题，让GBDT可以更好更快地用于工业实践。

#### 1.1 LightGBM在哪些地方进行了优化    (区别XGBoost)？

- 基于Histogram的决策树算法
- 带深度限制的Leaf-wise的叶子生长策略
- 直方图做差加速直接
- 支持类别特征(Categorical Feature)
- Cache命中率优化
- 基于直方图的稀疏特征优化多线程优化。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197431597512984.jpg)

#### 1.2 Histogram算法

直方图算法的基本思想是先把连续的浮点特征值离散化成k个整数（其实又是分桶的思想，而这些桶称为bin，比如[0,0.1)→0, [0.1,0.3)→1），同时构造一个宽度为k的直方图。

在遍历数据的时候，根据离散化后的值作为索引在直方图中累积统计量，当遍历一次数据后，直方图累积了需要的统计量，然后根据直方图的离散值，遍历寻找最优的分割点。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197418746568601.jpg)

使用直方图算法有很多优点。首先，最明显就是内存消耗的降低，直方图算法不仅不需要额外存储预排序的结果，而且可以只保存特征离散化后的值，而这个值一般用8位整型存储就足够了，内存消耗可以降低为原来的1/8。然后在计算上的代价也大幅降低，预排序算法每遍历一个特征值就需要计算一次分裂的增益，而直方图算法只需要计算k次（k可以认为是常数），时间复杂度从O(#data*#feature)优化到O(k*#features)。

#### 1.3 带深度限制的Leaf-wise的叶子生长策略

在XGBoost中，树是按层生长的，称为Level-wise tree growth，同一层的所有节点都做分裂，最后剪枝，如下图所示：

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197509149646916.png)

Level-wise过一次数据可以同时分裂同一层的叶子，容易进行多线程优化，也好控制模型复杂度，不容易过拟合。但实际上Level-wise是一种低效的算法，因为它不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。

在Histogram算法之上，LightGBM进行进一步的优化。首先它抛弃了大多数GBDT工具使用的按层生长 (level-wise)
的决策树生长策略，而使用了带有深度限制的按叶子生长 (leaf-wise)算法。

![](https://julyedu-img.oss-cn-beijing.aliyuncs.com/quesbase64155197520844369289.png)

Leaf-wise则是一种更为高效的策略，每次从当前所有叶子中，找到分裂增益最大的一个叶子，然后分裂，如此循环。因此同Level-wise相比，在分裂次数相同的情况下，Leaf-wise可以降低更多的误差，得到更好的精度。Leaf-wise的缺点是可能会长出比较深的决策树，产生过拟合。因此LightGBM在Leaf-wise之上增加了一个最大深度的限制，在保证高效率的同时防止过拟合。

#### 1.4 直方图差加速

LightGBM另一个优化是Histogram（直方图）做差加速。一个容易观察到的现象：一个叶子的直方图可以由它的父亲节点的直方图与它兄弟的直方图做差得到。通常构造直方图，需要遍历该叶子上的所有数据，但直方图做差仅需遍历直方图的k个桶。

利用这个方法，LightGBM可以在构造一个叶子的直方图后，可以用非常微小的代价得到它兄弟叶子的直方图，在速度上可以提升一倍。

#### 1.5 直接支持类别特征

实际上大多数机器学习工具都无法直接支持类别特征，一般需要把类别特征，转化到多维的0/1特征，降低了空间和时间的效率。而类别特征的使用是在实践中很常用的。基于这个考虑，LightGBM优化了对类别特征的支持，可以直接输入类别特征，不需要额外的0/1展开。并在决策树算法上增加了类别特征的决策规则。在Expo数据集上的实验，相比0/1展开的方法，训练速度可以加速8倍，并且精度一致。据我们所知，LightGBM是第一个直接支持类别特征的GBDT工具。

### 2. LightGBM优点

LightGBM （Light Gradient Boosting Machine）(请点击[https://github.com/Microsoft/LightGBM](https://github.com/Microsoft/LightGBM))是一个实现GBDT算法的框架，支持高效率的并行训练，并且具有以下优点：

- 更快的训练速度
- 更低的内存消耗
- 更好的准确率
- 分布式支持，可以快速处理海量数据
